{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c91f5a01-ea91-4ce3-9cf5-bf9eeb8ad8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv('normall.csv')\n",
    "test_data = pd.read_csv('anomlaydetection.csv')\n",
    "\n",
    "# Ensure test data has the same columns as train data\n",
    "X_test = test_data.drop(columns=['Anomaly'])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_data)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val = train_test_split(X_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06d6fee8-30e1-403a-ba33-eb9fc84e7c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_15\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_15\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_8       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │ input_layer_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │ dense_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span> │ dense_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span> │ dense_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                     │                   │            │ dense_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span> │ lambda_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span> │ dense_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ custom_variational… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CustomVariational…</span> │                   │            │ dense_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                     │                   │            │ dense_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                     │                   │            │ dense_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_8       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m160\u001b[0m │ input_layer_8[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │        \u001b[38;5;34m528\u001b[0m │ dense_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_50 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │         \u001b[38;5;34m34\u001b[0m │ dense_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_51 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │         \u001b[38;5;34m34\u001b[0m │ dense_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_8 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ dense_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                     │                   │            │ dense_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_52 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m48\u001b[0m │ lambda_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_53 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │         \u001b[38;5;34m68\u001b[0m │ dense_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ custom_variational… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ input_layer_8[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mCustomVariational…\u001b[0m │                   │            │ dense_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                     │                   │            │ dense_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                     │                   │            │ dense_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">872</span> (3.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m872\u001b[0m (3.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">872</span> (3.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m872\u001b[0m (3.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 825us/step - loss: 4.9879 - val_loss: 4.4693\n",
      "Epoch 2/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - loss: 4.5289 - val_loss: 4.1780\n",
      "Epoch 3/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462us/step - loss: 4.2075 - val_loss: 3.9826\n",
      "Epoch 4/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454us/step - loss: 4.1572 - val_loss: 3.9280\n",
      "Epoch 5/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455us/step - loss: 4.0359 - val_loss: 3.9134\n",
      "Epoch 6/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - loss: 3.9722 - val_loss: 3.8997\n",
      "Epoch 7/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 461us/step - loss: 4.0296 - val_loss: 3.8870\n",
      "Epoch 8/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469us/step - loss: 4.1054 - val_loss: 3.8759\n",
      "Epoch 9/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462us/step - loss: 4.0215 - val_loss: 3.8739\n",
      "Epoch 10/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458us/step - loss: 4.0941 - val_loss: 3.8765\n",
      "Epoch 11/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - loss: 4.0246 - val_loss: 3.8803\n",
      "Epoch 12/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465us/step - loss: 4.0537 - val_loss: 3.8700\n",
      "Epoch 13/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472us/step - loss: 4.0549 - val_loss: 3.8724\n",
      "Epoch 14/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468us/step - loss: 4.0102 - val_loss: 3.8704\n",
      "Epoch 15/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - loss: 3.9851 - val_loss: 3.8683\n",
      "Epoch 16/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458us/step - loss: 4.0088 - val_loss: 3.8670\n",
      "Epoch 17/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486us/step - loss: 4.0409 - val_loss: 3.8722\n",
      "Epoch 18/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458us/step - loss: 4.0651 - val_loss: 3.8645\n",
      "Epoch 19/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465us/step - loss: 4.0643 - val_loss: 3.8715\n",
      "Epoch 20/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 650us/step - loss: 4.0265 - val_loss: 3.8654\n",
      "Epoch 21/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465us/step - loss: 4.0912 - val_loss: 3.8759\n",
      "Epoch 22/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - loss: 4.0580 - val_loss: 3.8671\n",
      "Epoch 23/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458us/step - loss: 4.0441 - val_loss: 3.8681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x311403b90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import mse\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the VAE model\n",
    "input_dim = X_train.shape[1]\n",
    "latent_dim = 2  # Dimensionality of the latent space\n",
    "\n",
    "# Encoder\n",
    "inputs = Input(shape=(input_dim,))\n",
    "h = Dense(32, activation='relu')(inputs)\n",
    "h = Dense(16, activation='relu')(h)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# Decoder\n",
    "decoder_h = Dense(16, activation='relu')\n",
    "decoder_mean = Dense(input_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "# VAE model\n",
    "class CustomVariationalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, inputs, outputs, z_mean, z_log_var):\n",
    "        reconstruction_loss = mse(inputs, outputs)\n",
    "        reconstruction_loss *= input_dim\n",
    "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "        kl_loss = K.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        return K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, x_decoded_mean, z_mean, z_log_var = inputs\n",
    "        loss = self.vae_loss(x, x_decoded_mean, z_mean, z_log_var)\n",
    "        self.add_loss(loss)\n",
    "        return x\n",
    "\n",
    "# Use functional API to create the custom loss layer\n",
    "outputs = CustomVariationalLayer()([inputs, x_decoded_mean, z_mean, z_log_var])\n",
    "\n",
    "# Instantiate and compile the model\n",
    "vae = Model(inputs, outputs)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Train the model\n",
    "vae.fit(X_train, epochs=100, batch_size=32, validation_data=(X_val, None), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87272cef-35ca-47a7-b6d0-24585a169e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data columns: Index(['bearingA_x', 'bearingA_y', 'bearingB_x', 'bearingB_y'], dtype='object')\n",
      "Test data columns: Index(['bearingA_x', 'bearingA_y', 'bearingB_x', 'bearingB_y', 'Anomaly'], dtype='object')\n",
      "Epoch 1/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 1.2051 - val_loss: 6719175480722849792.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379us/step - loss: 1.0307 - val_loss: 108817866752.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - loss: 1.0183 - val_loss: 662738.5625\n",
      "Epoch 4/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384us/step - loss: 1.0037 - val_loss: 2264.4512\n",
      "Epoch 5/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - loss: 0.9952 - val_loss: 1796.6681\n",
      "Epoch 6/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372us/step - loss: 0.9918 - val_loss: 1736.7845\n",
      "Epoch 7/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372us/step - loss: 0.9937 - val_loss: 1730.4873\n",
      "Epoch 8/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 1.0149 - val_loss: 1728.9430\n",
      "Epoch 9/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378us/step - loss: 1.0178 - val_loss: 1727.2561\n",
      "Epoch 10/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372us/step - loss: 0.9967 - val_loss: 1726.5836\n",
      "Epoch 11/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - loss: 0.9905 - val_loss: 1726.1305\n",
      "Epoch 12/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380us/step - loss: 0.9933 - val_loss: 1726.1683\n",
      "Epoch 13/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378us/step - loss: 0.9937 - val_loss: 1726.0244\n",
      "Epoch 14/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372us/step - loss: 1.0158 - val_loss: 1726.0347\n",
      "Epoch 15/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - loss: 1.0080 - val_loss: 1725.9589\n",
      "Epoch 16/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - loss: 1.0005 - val_loss: 1725.9167\n",
      "Epoch 17/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589us/step - loss: 0.9959 - val_loss: 1725.8893\n",
      "Epoch 18/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step - loss: 1.0160 - val_loss: 1725.8510\n",
      "Epoch 19/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 0.9974 - val_loss: 1725.8456\n",
      "Epoch 20/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 0.9891 - val_loss: 1725.8257\n",
      "Epoch 21/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - loss: 0.9876 - val_loss: 1725.8259\n",
      "Epoch 22/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - loss: 1.0128 - val_loss: 1725.8169\n",
      "Epoch 23/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 1.0119 - val_loss: 1725.8010\n",
      "Epoch 24/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - loss: 1.0047 - val_loss: 1725.8002\n",
      "Epoch 25/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379us/step - loss: 0.9938 - val_loss: 1725.7955\n",
      "Epoch 26/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379us/step - loss: 1.0035 - val_loss: 1725.7943\n",
      "Epoch 27/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378us/step - loss: 0.9937 - val_loss: 1725.7979\n",
      "Epoch 28/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 1.0013 - val_loss: 1725.7917\n",
      "Epoch 29/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379us/step - loss: 1.0007 - val_loss: 1725.7838\n",
      "Epoch 30/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step - loss: 1.0106 - val_loss: 1725.7842\n",
      "Epoch 31/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378us/step - loss: 1.0158 - val_loss: 1725.7808\n",
      "Epoch 32/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step - loss: 0.9760 - val_loss: 1725.7877\n",
      "Epoch 33/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - loss: 0.9943 - val_loss: 1725.7848\n",
      "Epoch 34/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 1.0022 - val_loss: 1725.7797\n",
      "Epoch 35/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step - loss: 0.9896 - val_loss: 1725.7865\n",
      "Epoch 36/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602us/step - loss: 1.0094 - val_loss: 1725.7960\n",
      "Epoch 37/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378us/step - loss: 1.0236 - val_loss: 1725.7800\n",
      "Epoch 38/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 0.9973 - val_loss: 1725.7820\n",
      "Epoch 39/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379us/step - loss: 1.0022 - val_loss: 1725.7782\n",
      "Epoch 40/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 0.9802 - val_loss: 1725.7788\n",
      "Epoch 41/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - loss: 1.0096 - val_loss: 1725.7850\n",
      "Epoch 42/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 379us/step - loss: 1.0004 - val_loss: 1725.7761\n",
      "Epoch 43/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - loss: 1.0152 - val_loss: 1725.7762\n",
      "Epoch 44/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375us/step - loss: 0.9954 - val_loss: 1725.7820\n",
      "Epoch 45/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381us/step - loss: 0.9826 - val_loss: 1725.7827\n",
      "Epoch 46/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step - loss: 0.9920 - val_loss: 1725.7887\n",
      "Epoch 47/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - loss: 1.0070 - val_loss: 1725.7798\n",
      "Epoch 48/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374us/step - loss: 0.9922 - val_loss: 1725.7788\n",
      "Epoch 49/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377us/step - loss: 0.9869 - val_loss: 1725.7770\n",
      "Epoch 50/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380us/step - loss: 0.9871 - val_loss: 1725.7832\n",
      "Epoch 51/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 366us/step - loss: 1.0051 - val_loss: 1725.7793\n",
      "Epoch 52/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 376us/step - loss: 1.0140 - val_loss: 1725.7780\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Load and preprocess data\n",
    "train_data = pd.read_csv('normall.csv')\n",
    "test_data = pd.read_csv('anomalydetection.csv')\n",
    "\n",
    "# Print the column names to verify\n",
    "print(\"Train data columns:\", train_data.columns)\n",
    "print(\"Test data columns:\", test_data.columns)\n",
    "\n",
    "# Use all columns for X_train\n",
    "X_train = train_data.values\n",
    "\n",
    "# Drop the 'Anomaly' column from test_data and get X_test\n",
    "X_test = test_data.drop('Anomaly', axis=1).values\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Parameters\n",
    "input_dim = X_train.shape[1]\n",
    "latent_dim = 2\n",
    "intermediate_dim = 64\n",
    "\n",
    "# Sampling function for VAE\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# Define VAE model\n",
    "class VAE(Model):\n",
    "    def __init__(self, input_dim, latent_dim, intermediate_dim, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        \n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            Input(shape=(input_dim,)),\n",
    "            Dense(intermediate_dim, activation='relu'),\n",
    "            Dense(latent_dim + latent_dim)  # z_mean and z_log_var\n",
    "        ])\n",
    "        \n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            Input(shape=(latent_dim,)),\n",
    "            Dense(intermediate_dim, activation='relu'),\n",
    "            Dense(input_dim, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = tf.split(self.encoder(inputs), num_or_size_splits=2, axis=-1)\n",
    "        z = sampling([z_mean, z_log_var])\n",
    "        x_decoded_mean = self.decoder(z)\n",
    "        kl_loss = -0.5 * K.mean(z_log_var - K.square(z_mean) - K.exp(z_log_var) + 1, axis=-1)\n",
    "        reconstruction_loss = K.mean(K.square(inputs - x_decoded_mean), axis=-1)\n",
    "        vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        self.add_loss(vae_loss)\n",
    "        return x_decoded_mean\n",
    "\n",
    "# Instantiate and compile the model\n",
    "vae = VAE(input_dim=input_dim, latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
    "vae.compile(optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = vae.fit(X_train, epochs=100, batch_size=32, validation_data=(X_test, None), callbacks=[early_stopping])\n",
    "\n",
    "# Plot learning curves\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0eb805df-3f42-4ba7-bc6c-49474b0c0456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Number of anomalies detected: 30\n",
      "Total number of test samples: 595\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.24      1.00      0.39       138\n",
      "        True       1.00      0.07      0.12       457\n",
      "\n",
      "    accuracy                           0.28       595\n",
      "   macro avg       0.62      0.53      0.26       595\n",
      "weighted avg       0.82      0.28      0.19       595\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute reconstruction loss on the test data\n",
    "def compute_reconstruction_loss(model, X_test):\n",
    "    X_decoded_mean = model.predict(X_test)\n",
    "    reconstruction_loss = np.mean(np.square(X_test - X_decoded_mean), axis=-1)\n",
    "    return reconstruction_loss\n",
    "\n",
    "# Compute reconstruction loss for the test data\n",
    "reconstruction_loss = compute_reconstruction_loss(vae, X_test)\n",
    "\n",
    "# Optional: Set a threshold for anomaly detection\n",
    "# For example, you could use a percentile-based threshold\n",
    "threshold = np.percentile(reconstruction_loss, 95)  # Set threshold as 95th percentile\n",
    "\n",
    "# Identify anomalies based on reconstruction loss\n",
    "anomalies = reconstruction_loss > threshold\n",
    "\n",
    "# Print the number of anomalies\n",
    "print(f\"Number of anomalies detected: {np.sum(anomalies)}\")\n",
    "print(f\"Total number of test samples: {len(X_test)}\")\n",
    "\n",
    "# If the 'Anomaly' column in test_data has labels, you can compare\n",
    "if 'Anomaly' in test_data.columns:\n",
    "    true_anomalies = test_data['Anomaly'].values\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(true_anomalies, anomalies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bdaccdc1-b9da-4924-b4ad-9f822854c65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"encoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"encoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_47      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_163 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ input_layer_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_164 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │ dense_163[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_165 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │ dense_163[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_164[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ dense_165[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_47      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_163 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m320\u001b[0m │ input_layer_47[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_164 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │        \u001b[38;5;34m130\u001b[0m │ dense_163[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_165 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │        \u001b[38;5;34m130\u001b[0m │ dense_163[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_30 (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ dense_164[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ dense_165[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">580</span> (2.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m580\u001b[0m (2.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">580</span> (2.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m580\u001b[0m (2.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"decoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"decoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_166 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_167 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_48 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_166 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m192\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_167 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m260\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">452</span> (1.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m452\u001b[0m (1.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">452</span> (1.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m452\u001b[0m (1.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/var/folders/sc/vh_dt4bn43z02nv5hd85s7040000gn/T/ipykernel_6263/4235072280.py\", line 107, in train_step  *\n        tf.print(f\"Gradient {i} shape: {grad.shape}\")\n\n    AttributeError: 'NoneType' object has no attribute 'shape'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 129\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m    128\u001b[0m     batch_x \u001b[38;5;241m=\u001b[39m X_train[i \u001b[38;5;241m*\u001b[39m batch_size:(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_size]\n\u001b[0;32m--> 129\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Validation loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/sc/vh_dt4bn43z02nv5hd85s7040000gn/T/__autograph_generated_fileml16xrox.py:28\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(x_batch)\u001b[0m\n\u001b[1;32m     26\u001b[0m grad \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m i \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(i, grad)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_1\u001b[39m():\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ()\n",
      "File \u001b[0;32m/var/folders/sc/vh_dt4bn43z02nv5hd85s7040000gn/T/__autograph_generated_fileml16xrox.py:25\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloop_body\u001b[39m(itr):\n\u001b[1;32m     24\u001b[0m     i, grad \u001b[38;5;241m=\u001b[39m itr\n\u001b[0;32m---> 25\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mprint, (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGradient \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mag__\u001b[38;5;241m.\u001b[39mld(i)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/var/folders/sc/vh_dt4bn43z02nv5hd85s7040000gn/T/ipykernel_6263/4235072280.py\", line 107, in train_step  *\n        tf.print(f\"Gradient {i} shape: {grad.shape}\")\n\n    AttributeError: 'NoneType' object has no attribute 'shape'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(10)\n",
    "tf.random.set_seed(10)\n",
    "\n",
    "# Load data\n",
    "filename = \"normall.csv\"\n",
    "data = pd.read_csv(filename)\n",
    "\n",
    "# Split data\n",
    "train = data[0:3000]\n",
    "test = data[3000:4500]\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(train)\n",
    "X_test = scaler.transform(test)\n",
    "scaler_filename = \"scaler_data\"\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "# Define the VAE architecture\n",
    "input_dim = X_train.shape[1]\n",
    "latent_dim = 2\n",
    "\n",
    "# Encoder\n",
    "inputs = Input(shape=(input_dim,))\n",
    "h = Dense(64, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "# Decoder\n",
    "latent_inputs = Input(shape=(latent_dim,))\n",
    "h_decoded = Dense(64, activation='relu')(latent_inputs)\n",
    "outputs = Dense(input_dim, activation='sigmoid')(h_decoded)\n",
    "\n",
    "decoder = Model(latent_inputs, outputs, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "# Define Custom Loss Function\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Define the VAE loss function\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Define the VAE loss function\n",
    "def vae_loss_fn(inputs, x_decoded_mean):\n",
    "    z_mean, z_log_var = encoder(inputs)[0:2]\n",
    "    \n",
    "    # Convert inputs and predictions to the same data type\n",
    "    inputs = tf.cast(inputs, dtype=tf.float32)\n",
    "    x_decoded_mean = tf.cast(x_decoded_mean, dtype=tf.float32)\n",
    "    \n",
    "    # Compute the binary cross-entropy loss\n",
    "    xent_loss = input_dim * K.binary_crossentropy(inputs, x_decoded_mean)\n",
    "    \n",
    "    # Reduce the xent_loss to shape (batch_size,)\n",
    "    xent_loss = K.sum(xent_loss, axis=-1)\n",
    "    \n",
    "    # Compute the KL divergence loss\n",
    "    kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    \n",
    "    # Combine the losses\n",
    "    return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "# Define the custom training step\n",
    "@tf.function\n",
    "def train_step(x_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        z_mean, z_log_var, z = encoder(x_batch)\n",
    "        x_decoded_mean = decoder(z)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = vae_loss_fn(x_batch, x_decoded_mean)\n",
    "        \n",
    "    # Compute gradients\n",
    "    grads = tape.gradient(loss, vae.trainable_weights)\n",
    "    \n",
    "    # Debug prints\n",
    "    tf.print(\"Loss:\", loss)\n",
    "    for i, grad in enumerate(grads):\n",
    "        tf.print(f\"Gradient {i} shape: {grad.shape}\")\n",
    "    \n",
    "    if None in grads:\n",
    "        tf.print(\"Gradients:\", grads)\n",
    "        raise ValueError(\"One or more gradients are None.\")\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "    return loss\n",
    "# Compile the VAE model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "history = {'loss': [], 'val_loss': []}\n",
    "\n",
    "num_batches = X_train.shape[0] // batch_size\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(num_batches):\n",
    "        batch_x = X_train[i * batch_size:(i + 1) * batch_size]\n",
    "        loss = train_step(batch_x)\n",
    "        epoch_loss += loss.numpy()\n",
    "    \n",
    "    # Validation loss\n",
    "    val_loss = np.mean([vae_loss_fn(X_test, vae(X_test)).numpy()])\n",
    "    \n",
    "    # Store history\n",
    "    history['loss'].append(epoch_loss / num_batches)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {history['loss'][-1]:.4f} - Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(history['loss'], 'b', label='Train', linewidth=2)\n",
    "plt.plot(history['val_loss'], 'r', label='Validation', linewidth=2)\n",
    "plt.title('Model loss', fontsize=16)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "X_train_pred = vae.predict(X_train)\n",
    "X_test_pred = vae.predict(X_test)\n",
    "\n",
    "# Compute loss for training data\n",
    "train_loss = np.mean(np.abs(X_train_pred - X_train), axis=1)\n",
    "test_loss = np.mean(np.abs(X_test_pred - X_test), axis=1)\n",
    "\n",
    "# Compute threshold\n",
    "threshold = np.mean(train_loss) + 3 * np.std(train_loss)\n",
    "\n",
    "# Determine anomalies\n",
    "train_anomalies = train_loss > threshold\n",
    "test_anomalies = test_loss > threshold\n",
    "\n",
    "# Output results\n",
    "print(\"Training Loss Threshold:\", threshold)\n",
    "print(\"Number of Training Anomalies:\", np.sum(train_anomalies))\n",
    "print(\"Number of Test Anomalies:\", np.sum(test_anomalies))\n",
    "\n",
    "# Plot loss distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.histplot(train_loss, bins=20, kde=True, color='blue')\n",
    "plt.title('Training Loss Distribution')\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90965bf4-033e-4afa-b2eb-684bdb58e0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from seaborn) (3.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd8e7df-2d11-4651-a2ad-51aa3c612d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
