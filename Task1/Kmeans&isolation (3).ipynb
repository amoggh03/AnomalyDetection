{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1cc0649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of clusters: 2\n",
      "Distance threshold for anomalies: 4.759725249255874\n",
      "Is the normal sample an anomaly? True\n",
      "Is the anomalous sample an anomaly? False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Load the normal data\n",
    "df_normal = pd.read_csv('normall.csv')\n",
    "\n",
    "# Extract feature columns\n",
    "X_normal = df_normal[['bearingA_x', 'bearingA_y', 'bearingB_x', 'bearingB_y']].values\n",
    "\n",
    "# Standardize the data (important for k-means)\n",
    "scaler = StandardScaler()\n",
    "X_normal_scaled = scaler.fit_transform(X_normal)\n",
    "\n",
    "# Determine the optimal number of clusters using the silhouette score\n",
    "def find_optimal_k(data, k_range):\n",
    "    best_score = -1\n",
    "    best_k = None\n",
    "\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(data)\n",
    "        score = silhouette_score(data, labels)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "    return best_k\n",
    "\n",
    "# Define the range of k values to try\n",
    "k_range = range(2, 10)\n",
    "\n",
    "# Find the optimal number of clusters\n",
    "optimal_k = find_optimal_k(X_normal_scaled, k_range)\n",
    "print(f\"Optimal number of clusters: {optimal_k}\")\n",
    "\n",
    "# Train the k-means model with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "kmeans.fit(X_normal_scaled)\n",
    "\n",
    "# Calculate the distance threshold for anomalies\n",
    "# Here, we use the maximum distance to any centroid from the training data\n",
    "distances = kmeans.transform(X_normal_scaled)\n",
    "min_distances = distances.min(axis=1)\n",
    "threshold = min_distances.max()\n",
    "print(f\"Distance threshold for anomalies: {threshold}\")\n",
    "\n",
    "# Function to detect anomalies\n",
    "def is_anomaly(sample, kmeans_model, scaler, threshold):\n",
    "    sample_scaled = scaler.transform([sample])\n",
    "    distances = kmeans_model.transform(sample_scaled)\n",
    "    min_distance = distances.min(axis=1)[0]\n",
    "    return min_distance > threshold\n",
    "\n",
    "# Example test samples (assumed normal and anomalous samples)\n",
    "new_sample_normal = [-0.469915051151626, 0.275878711735454, -0.0275614966845328, -0.0887079422926568]  # Example normal data\n",
    "new_sample_anomaly = [0.189630843,0.012816744,-0.022304104,0.020324712] # Example anomalous data\n",
    "\n",
    "# Check if the samples are anomalies\n",
    "print(\"Is the normal sample an anomaly?\", is_anomaly(new_sample_normal, kmeans, scaler, threshold))\n",
    "print(\"Is the anomalous sample an anomaly?\", is_anomaly(new_sample_anomaly, kmeans, scaler, threshold))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ed4215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4999, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of clusters: 2\n",
      "Distance threshold for anomalies: 4.759725249255874\n",
      "(595, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the anomaly detection dataset: 1.00\n"
     ]
    }
   ],
   "source": [
    "#kmeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, accuracy_score\n",
    "\n",
    "df_normal = pd.read_csv('normall.csv')\n",
    "print(df_normal.shape)\n",
    "X_normal = df_normal[['bearingA_x', 'bearingA_y', 'bearingB_x', 'bearingB_y']].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_normal_scaled = scaler.fit_transform(X_normal)\n",
    "\n",
    "def find_optimal_k(data, k_range):\n",
    "    best_score = -1\n",
    "    best_k = None\n",
    "\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(data)\n",
    "        score = silhouette_score(data, labels)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "    return best_k\n",
    "\n",
    "k_range = range(2, 10)\n",
    "\n",
    "optimal_k = find_optimal_k(X_normal_scaled, k_range)\n",
    "print(f\"Optimal number of clusters: {optimal_k}\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "kmeans.fit(X_normal_scaled)\n",
    "\n",
    "\n",
    "distances = kmeans.transform(X_normal_scaled)\n",
    "min_distances = distances.min(axis=1)\n",
    "threshold = min_distances.max()\n",
    "print(f\"Distance threshold for anomalies: {threshold}\")\n",
    "\n",
    "def is_anomaly(sample, kmeans_model, scaler, threshold):\n",
    "    sample_scaled = scaler.transform([sample])\n",
    "    distances = kmeans_model.transform(sample_scaled)\n",
    "    min_distance = distances.min(axis=1)[0]\n",
    "    return min_distance > threshold\n",
    "\n",
    "df_anomaly = pd.read_csv('anomlaydetection.csv')\n",
    "print(df_anomaly.shape)\n",
    "X_anomaly = df_anomaly[['bearingA_x', 'bearingA_y', 'bearingB_x', 'bearingB_y']].values\n",
    "y_true = df_anomaly['Anomaly'].values\n",
    "\n",
    "X_anomaly_scaled = scaler.transform(X_anomaly)\n",
    "\n",
    "y_pred = [is_anomaly(sample, kmeans, scaler, threshold) for sample in X_anomaly]\n",
    "\n",
    "y_pred = np.array(y_pred, dtype=int)\n",
    "\n",
    "y_true = np.array(y_true, dtype=int)\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy of the model on the anomaly detection dataset: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dda10c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of      bearingA_x  bearingA_y  bearingB_x  bearingB_y\n",
      "0     -0.138363    0.028935   -0.019773   -0.002564\n",
      "1     -0.101087    0.012587   -0.005409    0.015794\n",
      "2     -0.105067   -0.003972    0.027830    0.026102\n",
      "3     -0.181645    0.080939   -0.012655    0.050237\n",
      "4     -0.153244    0.031137   -0.042393    0.046297\n",
      "..          ...         ...         ...         ...\n",
      "695   -0.025956    0.018290   -0.002704   -0.002209\n",
      "696   -0.017061   -0.001114    0.011258   -0.053202\n",
      "697    0.008415    0.011302    0.010797   -0.060995\n",
      "698    0.021866    0.014057   -0.018469   -0.014107\n",
      "699    0.041340   -0.001673    0.009491   -0.017855\n",
      "\n",
      "[700 rows x 4 columns]>\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "data=pandas.read_csv(\"normall.csv\")\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c12780af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the normal sample an anomaly? [False]\n",
      "Is the anomalous sample an anomaly? [ True]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Load the normal data\n",
    "df_normal = pd.read_csv('normall.csv')\n",
    "\n",
    "# Extract feature columns\n",
    "X_normal = df_normal[['bearingA_x', 'bearingA_y', 'bearingB_x', 'bearingB_y']].values\n",
    "\n",
    "# Standardize the data (important for Isolation Forest)\n",
    "scaler = StandardScaler()\n",
    "X_normal_scaled = scaler.fit_transform(X_normal)\n",
    "\n",
    "# Define the contamination rate (adjust as needed)\n",
    "contamination_rate = 0.02  # Experiment with different values\n",
    "\n",
    "# Train the Isolation Forest model with the contamination rate\n",
    "isolation_forest = IsolationForest(contamination=contamination_rate, random_state=42)\n",
    "isolation_forest.fit(X_normal_scaled)\n",
    "\n",
    "# Function to detect anomalies using Isolation Forest\n",
    "def is_anomaly_if(sample, isolation_forest_model, scaler):\n",
    "    sample_scaled = scaler.transform([sample])\n",
    "    prediction = isolation_forest_model.predict(sample_scaled)\n",
    "    return prediction == -1  # -1 indicates an anomaly according to Isolation Forest\n",
    "\n",
    "# Example test samples (assumed normal and anomalous samples)\n",
    "new_sample_normal = [ -0.138363, 0.028935 ,  -0.019773 ,  -0.002564]  # Example normal data\n",
    "new_sample_anomaly = [-0.323513804, 0.2427902, -0.564276067,-0.74642926] \n",
    "\n",
    "# Check if the samples are anomalies using Isolation Forest\n",
    "print(\"Is the normal sample an anomaly?\", is_anomaly_if(new_sample_normal, isolation_forest, scaler))\n",
    "print(\"Is the anomalous sample an anomaly?\", is_anomaly_if(new_sample_anomaly, isolation_forest, scaler))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f92bd8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#isolationforest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the normal data\n",
    "df_normal = pd.read_csv('Dataset/vibration_normal_5.csv')\n",
    "\n",
    "# Extract feature columns\n",
    "X_normal = df_normal[['bearingA_x', 'bearingA_y', 'bearingB_x', 'bearingB_y']].values\n",
    "\n",
    "# Standardize the data (important for Isolation Forest)\n",
    "scaler = StandardScaler()\n",
    "X_normal_scaled = scaler.fit_transform(X_normal)\n",
    "\n",
    "# Define the contamination rate (adjust as needed)\n",
    "contamination_rate = 0.02  # Experiment with different values\n",
    "\n",
    "# Train the Isolation Forest model with the contamination rate\n",
    "isolation_forest = IsolationForest(contamination=contamination_rate, random_state=42)\n",
    "isolation_forest.fit(X_normal_scaled)\n",
    "\n",
    "# Function to detect anomalies using Isolation Forest\n",
    "def is_anomaly_if(sample, isolation_forest_model, scaler):\n",
    "    sample_scaled = scaler.transform([sample])\n",
    "    prediction = isolation_forest_model.predict(sample_scaled)\n",
    "    return prediction == -1  # -1 indicates an anomaly according to Isolation Forest\n",
    "\n",
    "# Load the anomaly detection data\n",
    "df_anomaly = pd.read_csv('Dataset/anomlaydetection.csv')\n",
    "\n",
    "# Extract features and true labels\n",
    "X_anomaly = df_anomaly[['bearingA_x', 'bearingA_y', 'bearingB_x', 'bearingB_y']].values\n",
    "y_true = df_anomaly['Anomaly'].values\n",
    "\n",
    "# Standardize the anomaly detection data\n",
    "X_anomaly_scaled = scaler.transform(X_anomaly)\n",
    "\n",
    "# Predict anomalies\n",
    "y_pred = [is_anomaly_if(sample, isolation_forest, scaler) for sample in X_anomaly]\n",
    "\n",
    "# Convert boolean predictions to integers (0 for normal, 1 for anomaly)\n",
    "y_pred = np.array(y_pred, dtype=int)\n",
    "\n",
    "# Convert true labels to integers (0 for normal, 1 for anomaly)\n",
    "y_true = np.array(y_true, dtype=int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy of the model on the anomaly detection dataset: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d804ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_true, y_pred)\n",
    "# Calculate recall\n",
    "recall = recall_score(y_true, y_pred)\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
